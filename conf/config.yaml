
####################################################################
# hydra configs
####################################################################
hydra:
  run:
    dir: out/${model.name}/${experiment.name}/${now:%Y%m%d_%H%M%S}

####################################################################
# set configs
####################################################################
defaults:
  - model_conf@_global_: base
  - data_conf@_global_: app
  - exp_conf@experiment: eq
  - exp_conf/data_exp_conf@experiment: ${defaults.1.data_conf}_${defaults.2.exp_conf}

####################################################################
# data configs (default)
####################################################################
data:
  update: 20201230

  batch_size: 512
  sampling_freq: 20
  k_days: 20
  label_days: 20
  strategy_days: 250

  retrain_days: 240
  test_days: 5000  # test days
  init_train_len: 500
  train_data_len: 2000
  normalizing_window: 500  # norm windows for macro data_conf
  use_accum_data: True  # [sampler] 데이터 누적할지 말지

  random_guide_weight: 0.1
  random_flip: 0.1  # flip sign
  random_label: 0.1  # random sample from dist.

####################################################################
# model configs (default)
####################################################################
model:
  name: default

  d_model: 128
  dropout_r: 0.3

  mcdropout:
    in_dim: ${model.d_model}
    out_dim: ${data.num_assets}
    hidden_dim:
      - 128
      - 128
    dropout_r: ${model.dropout_r}
    mc_samples: 1000

  allocator:
    # num_assets: {wgt, mean, std} + d_model: {x_emb, attn output}
    in_dim: "(eval) ${data.num_assets} * 3 + ${model.d_model} * 2"
    out_dim: "(eval) ${data.num_assets} * 2"
    hidden_dim:
      - 128
      - 64

  ## attention
  attention:
    d_model: ${model.d_model}
    n_heads: 8
    d_k: "(eval) ${model.attention.d_model} // ${model.attention.n_heads}"
    d_v: "(eval) ${model.attention.d_model} // ${model.attention.n_heads}"
    d_ff: 128

  loss_list:
    - 'y_pf'
    - 'mdd_pf'
    - 'logy'
    - 'wgt_guide'
    - 'cost'

  pre_loss_wgt:
    'y_pf': 0.0
    'mdd_pf': 0.0
    'logy': 0.0
    'wgt_guide': 0.5
    'cost': 10.0

  loss_wgt:
    'y_pf': 1
    'mdd_pf': 1.0
    'logy': 1.0
    'wgt_guide': 0.02
    'cost': 1.0

####################################################################
# trainer configs (default)
####################################################################
trainer:
  stage1:
    monitor: 'val_loss'
    patience: 10

  stage2:
    monitor: 'val_loss'
    patience: 20

  gpus: 1
  check_val_every_n_epoch: 1  # 20
  gradient_clip_val: 1.

  auto_scale_batch_size: None  # None | power | binsearch

  adaptive_flag: True
  adaptive_count: 10
  es_max_count: 200
  loss_threshold: None  # -1
  plot_freq: 10
  save_freq: 20
  model_init_everytime: False

  num_epochs: 1000

####################################################################
# experiment configs (default)
####################################################################
experiment:
  ii: 3900
  seed: 1000
  pre_lr: 0.005
  lr: 0.05
  base_i0: 2000
  cost_rate: 0.003

  n_pretrain: 5
  use_guide_wgt_as_prev_x: False  # models / forward_with_loss

  log_level: DEBUG
#  outpath: ./out/${model.name}/${experiment.name}/{now:%Y%m%s_%H%M%S}
  outpath: .    # working directory가 알아서 바뀜... hydra.utils.to_absolute_path or utils.get_original_cwd()
