
####################################################################
# hydra configs
####################################################################
hydra:
  run:
    dir: out/${model.name}/${experiment.name}/${now:%Y%m%d_%H%M%S}

  sweep:
    dir: out/multirun/${experiment.name}
    subdir: ${now:%Y%m%d_%H%M%S}_${hydra.job.override_dirname}_${hydra.job.num}

####################################################################
# set configs
####################################################################
defaults:
  - model_conf@_global_: cash_first
  - data_conf@_global_: app
  - exp_conf@experiment: l
  - exp_conf/data_exp_conf@experiment: ${defaults.1.data_conf}_${defaults.2.exp_conf}
  - trainer_conf@_global_: stage2_losswgt_pf

####################################################################
# data configs (default)
####################################################################
data:
  update: 20201230

  batch_size: 512
  sampling_freq: 20
  k_days: 20
  label_days: 20
  strategy_days: 250

  retrain_days: 240
  test_days: 5000  # test days
  init_train_len: 500
  train_data_len: 2000
  normalizing_window: 500  # norm windows for macro data_conf
  use_accum_data: True  # [sampler] 데이터 누적할지 말지

  random_guide_weight: 0.1
  random_flip: 0.1  # flip sign
  random_label: 0.1  # random sample from dist.

####################################################################
# model configs (default)
####################################################################
model:
  name: default

  d_model: 128
  dropout_r: 0.3

  mcdropout:
    in_dim: ${model.d_model}
    out_dim: ${data.num_assets}
    hidden_dim:
      - 128
      - 128
    dropout_r: ${model.dropout_r}
    mc_samples: 1000

  allocator:
    # num_assets: {wgt, mean, std} + d_model: {x_emb, attn output}
    in_dim: "(eval) ${data.num_assets} * 3 + ${model.d_model} * 2"
    out_dim: "(eval) ${data.num_assets} * 2"
    hidden_dim:
      - 128
      - 64

  ## attention
  attention:
    d_model: ${model.d_model}
    n_heads: 8
    d_k: "(eval) ${model.attention.d_model} // ${model.attention.n_heads}"
    d_v: "(eval) ${model.attention.d_model} // ${model.attention.n_heads}"
    d_ff: 128


####################################################################
# trainer configs (default)
####################################################################
trainer:
  stage1:
    use: True
    monitor: 'val_loss'
    patience: 10
    lr: 0.005
    loss_wgt:
      'y_pf': 0.0
      'mdd_pf': 0.0
      'logy': 0.0
      'wgt_guide': 0.5
      'cost': 10.0

  stage2:
    use: True
    monitor: 'val_loss'
    patience: 20
    lr: ${experiment.lr}
    loss_wgt:
      'y_pf': 1
      'mdd_pf': 1.0
      'logy': 1.0
      'wgt_guide': 0.02
      'cost': 1.0

  gpus: 1
  check_val_every_n_epoch: 1  # 20
  gradient_clip_val: 1.
  max_epochs: 10000
  auto_scale_batch_size: None  # None | power | binsearch

  loss_threshold: None  # -1

####################################################################
# experiment configs (default)
####################################################################
experiment:
  ii: 3900
  seed: 1000

  # default
  base_i0: 2000
  cost_rate: 0.003
  lr: 0.05
  mdd_cp: 0.05

  use_guide_wgt_as_prev_x: False  # models / forward_with_loss
  plot_freq: 10

  # data related
  cash_idx: ${data.cash_idx}

#  outpath: ./out/${model.name}/${experiment.name}/{now:%Y%m%s_%H%M%S}
  outpath: .    # working directory가 알아서 바뀜... hydra.utils.to_absolute_path or utils.get_original_cwd()

logger:
  path_name: tb_logs
  name: my_model

